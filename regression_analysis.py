# -*- coding: utf-8 -*-
"""Regression Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UpebgqbdPoN-Q08QM1Xij1zuXc2Tc6ub
"""

## Importing essential libraries
import pandas as pd # to load the data,
import numpy as np # mathematical intitution

import seaborn as sns # ploting
import matplotlib.pyplot as plt # ploting

## Importing datasets
df = pd.read_csv('car_price.csv')

## To see top 5 rows of dataset
df.head()

## To see bottom 5 rows of dataset
df.tail()

## To see random 5 rows of dataset
df.sample(5)

df.Owner_Type.value_counts()

df['Name'].value_counts().any()

## To see the dataset information i.e column names, type of column, how many non-null values are present in the data
df.info()

df.columns

#We can see that in first column there is only index number given so its better to delete it
df=df.drop('Unnamed: 0',axis=1)

df.shape

## To get how many different types of data types are present in dataset
print("Unique Data type :",len(set(df.dtypes)))
print("Different types of datatypes in dataset are",set(df.dtypes))

## The car name with one frequency does not work for modeling purpose
df=df.groupby('Name').filter(lambda x : (x['Name'].value_counts()>1).any())

df.shape

## we know that mileage , power and Engine featurea are numeric but in dataset its units are also given So its datatype is object
## We will convert object type to data type
df["Engine"] = df["Engine"].str.replace(' CC','')
df['Power'] = df['Power'].str.replace('bhp', '')
df['Mileage'] = df['Mileage'].str.replace('kmpl', '')
df['Mileage'] = df['Mileage'].str.replace('km/kg', '')

## Now for numeric columns we can compute its description
df.describe()

"""### Missing Value"""

df.isnull().sum()

## In this column we have seen how many missing values are present in data
round(df.isnull().sum()/6019,2)

## This command is used to delete column from the dataframe
df=df.drop("New_Price",axis=1)

st = 'null'
st.replace('null', '74')

## Power column contain 'null' values which was not detected using isnull syntax
df['Power']=df.Power.replace('null ',df.Power.mode()[0])

## Mileage is a numeric variable which contain only 2 missing values so we are replacing it by median
df["Mileage"].fillna(df.Mileage.mode(), inplace=True)

df.shape

## Our data is big enough so we can delete missing values by using following syntax
df=df.dropna()
df.shape

"""### Duplicate Values"""

df = df.drop_duplicates()
df.shape

"""We can see that dataset does not contain duplicate values

### Outlier Detection
"""

## Outliers are present in numeric type column only so we are going to list numeric type columns
num_type = list(df.select_dtypes(include=['int64','float64']).columns)
print("Numbers of numeric type columns in data are" ,len(num_type))
print("Numeric type columns are " ,num_type)

## To visualize numeric type column boxplot is best way
num_features=['Year', 'Kilometers_Driven']

n = 1
plt.figure(figsize=(20,15))

for column in num_features:
    plt.subplot(4,4,n)
    n = n+1
    sns.boxplot(df[column])
    plt.tight_layout()

# df.Kilometers_Driven
q1 = np.percentile(df.Kilometers_Driven,25)
q3 = np.percentile(df.Kilometers_Driven,75)
iqr=q3-q1
print(iqr)
upper_limit=q3+1.5*iqr
print(upper_limit)
print(max(df.Kilometers_Driven))
print(q1-1.5*iqr)
min(df.Kilometers_Driven)

def outlier(column_name):
    """Outlier detection using Interquartile range"""
    q1=np.percentile(df[column_name],25)
    q3=np.percentile(df[column_name],75)
    iqr=q3-q1
    upper_limit=q3+1.5*iqr
    lower_limit=q1-1.5*iqr
    return upper_limit,lower_limit

## for year feature from boxplot we can see that outliers are only present in left side
upper_limit,lower_limit=outlier("Year")
##So replacing outliers by 2003
df.loc[df.Year < lower_limit, 'Year'] = 2006
## for Kilometers_Driven feature from boxplot we can see that outliers are only present in right side
upper_limit,lower_limit=outlier("Kilometers_Driven")
##So replacing outliers by its upper_limit
df.loc[df.Kilometers_Driven > upper_limit, 'Kilometers_Driven'] = upper_limit

df.shape

"""### Linear  regression assumptions
- Multicollinearity
"""

df.corr()

## Plot the heatmap to see correlation with columns
fig, ax = plt.subplots(figsize=(12,8))
sns.heatmap(df.corr(), annot=True, ax=ax);

"""We can observe that All auto correlations are less than 0.60 . So using auto correlation function we can say there is no multicollinearity. But still we are going to check using VIF."""

## To check Multicollinearity here we are using VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor

X = df[list(df.select_dtypes(include=['int64','float64']).columns)]
# Price feature is dependent or o/p feature so we are deleting
X=X.drop('Price',axis=1)

# VIF dataframe
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns

# calculating VIF for each feature
vif_data["VIF"] = [variance_inflation_factor(X.values, i)
                          for i in range(len(X.columns))]

print(vif_data)

"""here we will delete seats feature"""

df=df.drop('Seats',axis=1)

X = df[list(df.select_dtypes(include=['int64','float64']).columns)]
X=X.drop('Price',axis=1)

# VIF dataframe
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns

# calculating VIF for each feature
vif_data["VIF"] = [variance_inflation_factor(X.values, i)
                          for i in range(len(X.columns))]

print(vif_data)

"""Now all VIF values are less than 10 after deleting Seats column.So no multicollinearity present in data.

### Converting Categorical variables to Numerical

LabelEncoder
one hot enoding | dummy variable
# one way
1st obs--> location_hyd= 1 | location_pune=0 | location_mum=0 | location_delhi=0
2nd obs--> location_hyd= 0 | location_pune=0 | location_mum=0 | location_delhi=1

# 2nd way
1st obs--> 1
2nd obs--> 4

## method 1 - label encoding
"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

## Name column has lots of unique values so we are lable encoding here
df['Name']=le.fit_transform(df['Name'])
df['Engine']=le.fit_transform(df['Engine'])
df['Mileage']=le.fit_transform(df['Mileage'])

"""## method 2 - one hot enoding or dummy variable"""

## creating list of column names with dtype object
categorical_features = list(df.columns[df.dtypes == object])
categorical_features

## All other columns are nominal type so converting categorical variable to numeric using get_dummy
df=pd.get_dummies(df, columns = categorical_features)
df.head()

df.sample(5)

df.shape

"""## Train Test Split"""

from sklearn.model_selection import train_test_split

x=df.drop('Price',axis=1)
y=df['Price']
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size =0.1)

##Standardization of data means convering values in between 0-1. In regression Standardization is important because model is sensitive to outliers.
## Standardizing the data
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(x_train)
X_test = sc_X.transform(x_test)

print('x_train',x_train.shape)
print('y_train',y_train.shape)
print('x_test',x_test.shape)
print('y_test',y_test.shape)

"""### Model Building"""

from sklearn.linear_model import LinearRegression
model= LinearRegression()

model.fit(x_train, y_train)

"""R square :It is statistical measure of how close the data are to the fitted regression line."""

r_sq = model.score(x_train , y_train)
print('coefficient of determination:', r_sq)

## It will give output of x_test
y_pred = model.predict(x_test)

# importing r2_score module
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
# predicting the accuracy score
score=r2_score(y_pred,y_test)
print('r2 socre is' ,score)
print('mean_sqrd_error is==',mean_squared_error(y_test,y_pred))
print('root_mean_squared error of is==',np.sqrt(mean_squared_error(y_test,y_pred)))